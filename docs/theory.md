# Theory: Machine Theory of Mind (MToM)

## 1. Overview

This document formalizes the theoretical foundations of the proposed Machine Theory of Mind (MToM) framework. The goal of MToM is to equip artificial agents with explicit, interpretable models of how humans form beliefs, infer intentions, and evaluate social behavior, and to incorporate these models directly into decision-making.

Unlike purely behavioral or reward-driven approaches, MToM treats social reasoning as a latent inference problem: agents maintain and update beliefs about human mental states and optimize actions with respect to both task performance and social intelligence.

## 2. Problem Setting

We consider sequential interaction settings involving:

- an artificial agent $A$,
- one or more human or simulated observers $H$,
- and an environment $E$ producing observable outcomes.

At each time step $t$:

- the agent selects an action $a_t \in A$,
- observers update their internal beliefs about the agent,
- and the agent receives both task-related and social feedback.

The agent's objective is not only to succeed at the task, but to reason about how its actions are interpreted.

## 3. Mental-State Hypothesis Space

Let

$$H = \{h_1, \ldots, h_m\}$$

be a finite hypothesis space representing latent human mental states. In this work, hypotheses correspond to discrete bins or continuous regions over interpretable social dimensions such as:

- **Warmth** (perceived cooperativeness, benevolence),
- **Competence** (perceived capability, effectiveness).

Each hypothesis $h \in H$ induces a likelihood model

$$P(o \mid h, a),$$

describing how a human observer would respond to agent action $a$.

## 4. Bayesian Theory of Mind

### 4.1 Belief Representation

The agent maintains a belief distribution

$$P_t(h) = P(h \mid o_{1:t}),$$

updated after each observation $o_t$.

### 4.2 Belief Updates

Beliefs are updated via Bayes' rule:

$$P_{t+1}(h) \propto P(o_t \mid h, a_t) \, P_t(h),$$

with priors initialized to have full support over $H$.

This enables explicit tracking of uncertainty and interpretability of social inference.

## 5. Posterior Consistency

**Theorem 1 (Posterior Concentration)**

Let $H$ be finite. Suppose:

1. The observation model is identifiable: for any $h \neq h^*$, there exists an observation $o$ such that $P(o \mid h) \neq P(o \mid h^*)$.
2. All hypotheses assign nonzero probability to feasible observations.
3. The prior has full support.

Then, if observations are generated by the true hypothesis $h^*$,

$$\lim_{t \to \infty} P(h^* \mid o_{1:t}) = 1 \quad \text{almost surely}.$$

**Interpretation**

Under standard assumptions, the agent's social beliefs converge to the correct mental-state hypothesis. This establishes epistemic validity of the Bayesian ToM module.

**Empirical Hook**

Posterior trajectories logged in Week 4 and Week 7 experiments show exponential decay of incorrect hypotheses, consistent with this result.

## 6. Social Decision-Making as Multi-Objective Optimization

### 6.1 Objectives

Define:

- $R(\pi)$: expected task reward under policy $\pi$,
- $S(\pi)$: expected Social Intelligence Quotient (SIQ).

The agent seeks to optimize:

$$J(\pi) = (R(\pi), S(\pi)).$$

### 6.2 Pareto Optimality

A policy $\pi_a$ dominates $\pi_b$ if:

$$R(\pi_a) \geq R(\pi_b), \quad S(\pi_a) \geq S(\pi_b),$$

with at least one strict inequality.

A policy is **Pareto optimal** if no other policy dominates it.

## 7. Scalarization Sufficiency

**Proposition 2 (Scalarization)**

Assume SIQ is monotone in its components. Then every Pareto-optimal policy is a solution to:

$$\max_\pi (1 - \lambda) R(\pi) + \lambda S(\pi)$$

for some $\lambda \in [0, 1]$.

Conversely, every optimizer of the scalarized objective is Pareto optimal.

**Interpretation**

The social-weight parameter $\lambda$ traces the empirical Pareto frontier between task performance and social intelligence.

**Empirical Hook**

Week 3 and Week 5 Pareto plots demonstrate this trade-off explicitly, with Bayesian MToM policies occupying the efficient frontier.

## 8. First-Order Social Improvement

### Setup

Consider an entropy-regularized objective:

$$J_\lambda(\pi) = \mathbb{E}_\pi[R(a)] + \lambda \mathbb{E}_\pi[\Delta_{\text{obs}}(a)] - \tau \, \text{KL}(\pi \| \pi_0),$$

where:

- $\Delta_{\text{obs}}(a)$ denotes predicted social impact,
- $\tau > 0$ controls policy smoothness.

**Theorem 3 (Small-$\lambda$ Improvement)**

If:

1. The observer model is correct,
2. $\Delta_{\text{obs}}$ has nonzero variance under $\pi_0$,
3. The policy is differentiable at $\lambda = 0$,

then for sufficiently small $\lambda$:

$$S(\pi_\lambda) \geq S(\pi_0) + \frac{\lambda}{\tau} \text{Var}_{\pi_0}[\Delta_{\text{obs}}] - O(\lambda^2).$$

**Interpretation**

Even a small social incentive yields a guaranteed first-order increase in social intelligence, provided the agent's observer model is accurate.

**Empirical Hook**

Week 7 Î»-micro-sweep experiments confirm the predicted linear regime.

## 9. Role of Priors and Norms

Social-norm priors encode cultural and contextual expectations over mental states. These priors:

- regularize belief updates,
- stabilize behavior under noisy or adversarial feedback,
- and improve robustness and ethical consistency.

Empirically, moderate prior strength maximizes both utility and SIQ, as shown in Week 5 sweeps.

## 10. Interpretability and Explainability

Because beliefs are explicit probability distributions over interpretable dimensions, the MToM framework supports:

- inspection of belief trajectories,
- counterfactual reasoning ("what belief led to this action?"),
- and transparent explanation of social decisions.

This distinguishes MToM from purely neural or behavior-cloning approaches.

## 11. Summary

The Machine Theory of Mind framework formalizes social reasoning as Bayesian belief inference + multi-objective optimization. Theoretical guarantees establish:

- consistency of belief updates,
- sufficiency of scalarized objectives,
- and guaranteed first-order social gains.

Together with empirical validation, this theory provides a principled foundation for socially intelligent, interpretable AI systems.
