# Human Pilot Survey Protocol

## Overview
This pilot invites participants to read short dialogues between a human and two versions of an agent (MToM vs baseline) and rate each in terms of warmth, competence, and trust. The interface is served by `demo/human_pilot_app.py`, which randomizes dialogue order per participant, collects the three sliders, and records each response with a completion code for Prolific verification.

## Consent text
Participants see this text before the survey begins:

> **Welcome!**
>
> Youâ€™ll read short dialogues with AI assistants and rate them on warmth, competence, and trust.
> The study is anonymous, takes under 7 minutes, and stores no personal data.
> By clicking **Start**, you agree to participate voluntarily.

## Procedure
1. **Start screen** â€“ Participants land on the consent/instructions page above and confirm understanding by clicking **Start the Study**.
2. **Dialogue experience** â€“ Dialogues are loaded one at a time. Every dialogue is small (2-3 turns) and is displayed inside a bordered text box with human (`ğŸ‘¤`) and agent (`ğŸ¤–`) labels. The rating section displays three sliders, each ranging 1â€“7, with explicit labels describing what high/low values mean.
3. **Rating questions** â€“ For each dialogue, participants answer:
   - Warmth â€“ â€œHow friendly or empathetic does this agent seem?â€
   - Competence â€“ â€œHow capable or intelligent does this agent seem?â€
   - Trust â€“ â€œHow much would you trust this agent in a future interaction?â€
4. **Progress tracking** â€“ A badge at the top shows the current dialogue number (e.g., â€œDialogue 5 of 12â€). Clicking **Next Dialogue â¡ï¸** saves the scores and loads the next snippet until all dialogues are rated.
5. **Completion code** â€“ After the final dialogue everyone sees a thank-you banner plus a completion code such as `HUMTOM4821` that participants can paste into Prolific to prove they finished the study.

## Data handling
Responses are appended to `data/human_pilot/pilot_ratings.csv` with the following columns:
- `timestamp` â€“ UTC timestamp of submission
- `dialogue_id` â€“ identifier for the dialogue (e.g., `MTOM-01`)
- `agent_type` â€“ either `MToM` or `Baseline`
- `warmth`, `competence`, `trust` â€“ slider values (1â€“7)
- `completion_code` â€“ generated code used for participant verification

A copy of the dataset is written to `results/week10/pilot_ratings_combined.csv` and the averages by agent type are stored in `results/week10/agent_means.csv`. `tools/analyze_human_pilot.py` reloads the saved CSV, recomputes averages, and produces a bar chart (`results/week10/agent_comparison.png`).

## Ethical considerations
- The study logs no personal identifiers. Only timestamps, dialogue IDs, and generated completion codes are stored.
- Warmth/competence/trust sliders are anchored with explicit high/low text so participants know how to interpret the scale.
- Participants may stop at any time; partial responses are not captured because each screen transition saves only after the sliders are submitted and the **Next** button is clicked.
